{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38820215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2002-05-05', '2002-05-06', '2002-05-07', '2002-05-08',\n",
      "               '2002-05-09', '2002-05-10', '2002-05-11', '2002-05-12',\n",
      "               '2002-05-13', '2002-05-14',\n",
      "               ...\n",
      "               '2016-12-22', '2016-12-23', '2016-12-24', '2016-12-25',\n",
      "               '2016-12-26', '2016-12-27', '2016-12-28', '2016-12-29',\n",
      "               '2016-12-30', '2016-12-31'],\n",
      "              dtype='datetime64[ns]', name='time', length=5355, freq=None) DatetimeIndex(['2002-06-04', '2002-06-11', '2002-06-18', '2002-06-25',\n",
      "               '2002-07-02', '2002-07-09', '2002-07-16', '2002-07-23',\n",
      "               '2002-07-30', '2002-08-06',\n",
      "               ...\n",
      "               '2015-10-27', '2015-11-03', '2015-11-10', '2015-11-17',\n",
      "               '2015-11-24', '2015-12-01', '2015-12-08', '2015-12-15',\n",
      "               '2015-12-22', '2015-12-29'],\n",
      "              dtype='datetime64[ns]', name='Date', length=709, freq=None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras_tuner as kt\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from numpy import empty\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from os import chdir\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import math\n",
    "\n",
    "csv_file_path = 'D:/Arbeit PhD/Fachlich/z_Sonstiges/Groundwater challenge/data/Sweden_2'\n",
    "chdir(csv_file_path)\n",
    "\n",
    "Y_all = pd.read_csv('heads.csv',decimal='.',index_col=0, delimiter=',', header=0,parse_dates=True)\n",
    "X_all = pd.read_csv('input_data.csv',decimal='.',index_col='time', delimiter=',', header=0,parse_dates=True)\n",
    "\n",
    "#print(Y_all)\n",
    "#print(X_all)\n",
    "Y = Y_all['2002-06-04':' 2016-12-31']\n",
    "X = X_all['2002-05-05':' 2016-12-31']\n",
    "# 2017-01-01 to 2021-12-31 testing # we don't have target variables for the \"testing phase\"\n",
    "# for sweden weekly forecasts are asked\n",
    "print(X.index, Y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af3e234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2002-05-05', '2002-05-06', '2002-05-07', '2002-05-08',\n",
      "               '2002-05-09', '2002-05-10', '2002-05-11', '2002-05-12',\n",
      "               '2002-05-13', '2002-05-14',\n",
      "               ...\n",
      "               '2011-12-17', '2011-12-18', '2011-12-19', '2011-12-20',\n",
      "               '2011-12-21', '2011-12-22', '2011-12-23', '2011-12-24',\n",
      "               '2011-12-25', '2011-12-26'],\n",
      "              dtype='datetime64[ns]', name='time', length=3523, freq=None) DatetimeIndex(['2002-06-04', '2002-06-11', '2002-06-18', '2002-06-25',\n",
      "               '2002-07-02', '2002-07-09', '2002-07-16', '2002-07-23',\n",
      "               '2002-07-30', '2002-08-06',\n",
      "               ...\n",
      "               '2011-10-25', '2011-11-01', '2011-11-08', '2011-11-15',\n",
      "               '2011-11-22', '2011-11-29', '2011-12-06', '2011-12-13',\n",
      "               '2011-12-20', '2011-12-27'],\n",
      "              dtype='datetime64[ns]', name='Date', length=500, freq=None)\n",
      "DatetimeIndex(['2014-01-05', '2014-01-06', '2014-01-07', '2014-01-08',\n",
      "               '2014-01-09', '2014-01-10', '2014-01-11', '2014-01-12',\n",
      "               '2014-01-13', '2014-01-14',\n",
      "               ...\n",
      "               '2014-12-20', '2014-12-21', '2014-12-22', '2014-12-23',\n",
      "               '2014-12-24', '2014-12-25', '2014-12-26', '2014-12-27',\n",
      "               '2014-12-28', '2014-12-29'],\n",
      "              dtype='datetime64[ns]', name='time', length=359, freq=None) DatetimeIndex(['2014-02-04', '2014-02-11', '2014-02-18', '2014-02-25',\n",
      "               '2014-03-04', '2014-03-11', '2014-03-18', '2014-03-25',\n",
      "               '2014-04-01', '2014-04-08', '2014-04-15', '2014-04-22',\n",
      "               '2014-04-29', '2014-05-06', '2014-05-13', '2014-05-20',\n",
      "               '2014-05-27', '2014-06-03', '2014-06-10', '2014-06-17',\n",
      "               '2014-06-24', '2014-07-01', '2014-07-08', '2014-07-15',\n",
      "               '2014-07-22', '2014-07-29', '2014-08-05', '2014-08-12',\n",
      "               '2014-08-19', '2014-08-26', '2014-09-02', '2014-09-09',\n",
      "               '2014-09-16', '2014-09-23', '2014-09-30', '2014-10-07',\n",
      "               '2014-10-14', '2014-10-21', '2014-10-28', '2014-11-04',\n",
      "               '2014-11-11', '2014-11-18', '2014-11-25', '2014-12-02',\n",
      "               '2014-12-09', '2014-12-16', '2014-12-23', '2014-12-30'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n",
      "(500, 1) (48, 1) (101, 1)\n",
      "(500, 1) y_train.shape transformed\n",
      "(359, 9)\n",
      "(3523, 9)\n",
      "(730, 9)\n",
      "(48, 30, 9) (500, 30, 9) (101, 30, 9)\n",
      "(48, 1) (500, 1) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:'2011-12-26']\n",
    "Y_train = Y[:'2011-12-27']\n",
    "print(X_train.index, Y_train.index)\n",
    "\n",
    "X_test = X['2014-01-05':'2014-12-29']\n",
    "Y_test = Y['2014-02-04':'2014-12-30']#29\n",
    "print(X_test.index, Y_test.index)\n",
    "\n",
    "X_valid = X['2012-01-01':'2013-12-30']\n",
    "Y_valid = Y['2012-01-31':'2013-12-31']\n",
    "#print(X_valid.index, Y_valid.index)\n",
    "print(Y_train.shape, Y_test.shape, Y_valid.shape)\n",
    "\n",
    "csv_file_path = 'D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge\\data'\n",
    "chdir(csv_file_path)\n",
    "from helper import *\n",
    "\n",
    "#Preprocessing nur Warning, kein Fehler, normierung wird korrekt durchgef√ºhrt\n",
    "(x_tr,y, x_te, y_test, x_va, y_valid, scaler_X,scaler_Y)=prepare_data_weekly(X_train,Y_train,X_valid,Y_valid,X_test,Y_test)\n",
    "print(x_te.shape)\n",
    "print(x_tr.shape)\n",
    "print(x_va.shape)\n",
    "\n",
    "iters=3\n",
    "n_steps_in=30\n",
    "n_steps_out=1\n",
    "\n",
    "x = split_sequences_x(x_tr, n_steps_in, n_steps_out)\n",
    "x_test = split_sequences_x(x_te, n_steps_in, n_steps_out)\n",
    "x_valid = split_sequences_x(x_va, n_steps_in, n_steps_out)\n",
    "print(x_test.shape, x.shape, x_valid.shape)\n",
    "print(y_test.shape, y.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "465050a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer code is from keras example on time series classification\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "class Dropout(keras.layers.Dropout):\n",
    "    def __init__(self, rate, training=None, noise_shape=None, seed=None, **kwargs):\n",
    "        super(Dropout, self).__init__(rate, noise_shape=None, seed=None,**kwargs)\n",
    "        self.training = training\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed)\n",
    "            if not training: \n",
    "                return K.in_train_phase(dropped_inputs, inputs, training=self.training)\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        return inputs\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x, training=True)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x, training=True)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    print('test1')\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    print('test2')\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)#change here enc/dec\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in [mlp_units]:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x, training=True)\n",
    "    outputs = layers.Dense(output_shape, activation=\"relu\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c88ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Transformer_HyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(Transformer_HyperModel, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "    def build(self, hp):#statt build_model\n",
    "        head_size = hp.Int(\"head_size\", min_value=8, max_value=64) #, default=16\n",
    "        num_heads = 1\n",
    "        ff_dim = hp.Int(\"ff_dim\", min_value=1, max_value=8) #, default=4\n",
    "        num_transformer_blocks = 2 #, default=4\n",
    "        mlp_units = hp.Int(\"mlp_units\", min_value=10, max_value=200)\n",
    "        dropout = 0.1\n",
    "        mlp_dropout = 0.1\n",
    "        \n",
    "        model = build_model(self.input_shape, self.output_shape, head_size,num_heads,ff_dim,\n",
    "                                num_transformer_blocks,mlp_units,mlp_dropout)\n",
    "        model.compile(loss=\"mean_absolute_error\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),metrics=[\"mean_absolute_error\"])\n",
    "        return model\n",
    "\n",
    "input_shape = x.shape[1:]\n",
    "output_shape = y.shape[1]\n",
    "\n",
    "hyper_model = Transformer_HyperModel(input_shape, output_shape)\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error',restore_best_weights=True, patience=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed3bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 05s]\n",
      "val_mean_absolute_error: 0.1664285510778427\n",
      "\n",
      "Best val_mean_absolute_error So Far: 0.13872769474983215\n",
      "Total elapsed time: 00h 01m 34s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 30, 9)        18          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 30, 9)        2427        layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 30, 9)        0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 30, 9)        0           dropout[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 30, 9)        18          tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 30, 8)        80          layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 8)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 30, 9)        81          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 30, 9)        0           conv1d_1[0][0]                   \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 30, 9)        18          tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 30, 9)        2427        layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30, 9)        0           multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 30, 9)        0           dropout_2[0][0]                  \n",
      "                                                                 tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 30, 9)        18          tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 30, 8)        80          layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 30, 8)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 30, 9)        81          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 30, 9)        0           conv1d_3[0][0]                   \n",
      "                                                                 tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 30)           0           tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 156)          4836        global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 156)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            157         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,241\n",
      "Trainable params: 10,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner= kt.RandomSearch(\n",
    "        hyper_model,\n",
    "        overwrite=True,\n",
    "        objective='val_mean_absolute_error',\n",
    "        max_trials = 10,\n",
    "        directory='D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge',\n",
    "        project_name='TF Sweden2'\n",
    "        )\n",
    "\n",
    "tuner.search(\n",
    "        x,\n",
    "        y,\n",
    "        batch_size=24,\n",
    "        epochs=30,\n",
    "        validation_data=(x_valid,y_valid),#achtung test-set hier darf nicht mein standard test set sein, sondern val-set!! \n",
    "        callbacks=[es_callback],\n",
    "        verbose=1)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "Y_pred=best_model.predict(x_test)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f4f5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge\\TF Sweden2\n",
      "Showing 10 best trials\n",
      "Objective(name='val_mean_absolute_error', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 62\n",
      "ff_dim: 8\n",
      "mlp_units: 156\n",
      "Score: 0.13872769474983215\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 14\n",
      "ff_dim: 2\n",
      "mlp_units: 61\n",
      "Score: 0.14910650253295898\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 35\n",
      "ff_dim: 1\n",
      "mlp_units: 68\n",
      "Score: 0.16291871666908264\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 23\n",
      "ff_dim: 4\n",
      "mlp_units: 161\n",
      "Score: 0.1664285510778427\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 23\n",
      "ff_dim: 2\n",
      "mlp_units: 60\n",
      "Score: 0.16745036840438843\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 36\n",
      "ff_dim: 2\n",
      "mlp_units: 56\n",
      "Score: 0.18067550659179688\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 28\n",
      "ff_dim: 2\n",
      "mlp_units: 37\n",
      "Score: 0.18742665648460388\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 29\n",
      "ff_dim: 5\n",
      "mlp_units: 41\n",
      "Score: 0.20826636254787445\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 9\n",
      "ff_dim: 7\n",
      "mlp_units: 184\n",
      "Score: 0.21682465076446533\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 61\n",
      "ff_dim: 2\n",
      "mlp_units: 23\n",
      "Score: 0.40603935718536377\n",
      "0.5716820017496774\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()\n",
    "\n",
    "csv_file_path = 'D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge'\n",
    "chdir(csv_file_path)\n",
    "from tensorflow.keras.models import save_model\n",
    "best_model.save('TF_Sweden2_tune.h5')\n",
    "\n",
    "NSE=np.array([[None]*n_steps_out])\n",
    "MAE=np.array([[None]*n_steps_out])\n",
    "RMSE=np.array([[None]*n_steps_out])\n",
    "SAPE=np.array([[None]*n_steps_out])\n",
    "\n",
    "Y_pred_test=best_model.predict(x_test)\n",
    "Y_pred_valid=best_model.predict(x_valid)\n",
    "\n",
    "y_correct_utf = scaler_Y.inverse_transform(y_test)#test\n",
    "yhat_utf = scaler_Y.inverse_transform(Y_pred_test)#test\n",
    "Y_pred_valid_utf = scaler_Y.inverse_transform(Y_pred_valid)#valid\n",
    "\n",
    "NSE[:]= np.array([1 - sum((yhat_utf[:,i]-y_correct_utf[:,i])**2)/sum((y_correct_utf[:,i]-np.mean(y_correct_utf[:,i]))**2)\n",
    "                  for i in range(n_steps_out)])\n",
    "MAE[:]=np.array([mean_absolute_error(y_correct_utf[:,i], yhat_utf[:,i]) for i in range(n_steps_out)])\n",
    "RMSE[:]=np.array([np.sqrt(np.mean((yhat_utf[:,i]-y_correct_utf[:,i])**2)) for i in range(n_steps_out)])\n",
    "SAPE[:] = np.array([np.abs((yhat_utf[:,i] - y_correct_utf[:,i])/(0.5*(y_correct_utf[:,i]+yhat_utf[:,i]))).mean()\n",
    "                         for i in range(n_steps_out)]) #mean absolute percent error\n",
    "\n",
    "print(np.mean(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e440e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27feac42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307092c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41765479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2dfcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8362c94acb70529fb597ced4e020ad9ec7f0e835666f207c4442104931f7034b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
