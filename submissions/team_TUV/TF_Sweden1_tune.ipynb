{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38820215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2002-05-05', '2002-05-06', '2002-05-07', '2002-05-08',\n",
      "               '2002-05-09', '2002-05-10', '2002-05-11', '2002-05-12',\n",
      "               '2002-05-13', '2002-05-14',\n",
      "               ...\n",
      "               '2016-12-22', '2016-12-23', '2016-12-24', '2016-12-25',\n",
      "               '2016-12-26', '2016-12-27', '2016-12-28', '2016-12-29',\n",
      "               '2016-12-30', '2016-12-31'],\n",
      "              dtype='datetime64[ns]', name='time', length=5355, freq=None) DatetimeIndex(['2002-06-04', '2002-06-11', '2002-06-18', '2002-06-25',\n",
      "               '2002-07-02', '2002-07-09', '2002-07-16', '2002-07-23',\n",
      "               '2002-07-30', '2002-08-06',\n",
      "               ...\n",
      "               '2015-10-27', '2015-11-03', '2015-11-10', '2015-11-17',\n",
      "               '2015-11-24', '2015-12-01', '2015-12-08', '2015-12-15',\n",
      "               '2015-12-22', '2015-12-29'],\n",
      "              dtype='datetime64[ns]', name='Date', length=709, freq=None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras_tuner as kt\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from numpy import empty\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from os import chdir\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import math\n",
    "\n",
    "csv_file_path = 'D:/Arbeit PhD/Fachlich/z_Sonstiges/Groundwater challenge/data/Sweden_1'\n",
    "chdir(csv_file_path)\n",
    "\n",
    "Y_all = pd.read_csv('heads.csv',decimal='.',index_col=0, delimiter=',', header=0,parse_dates=True)\n",
    "X_all = pd.read_csv('input_data.csv',decimal='.',index_col='time', delimiter=',', header=0,parse_dates=True)\n",
    "\n",
    "Y = Y_all['2002-06-04':' 2016-12-31']\n",
    "X = X_all['2002-05-05':' 2016-12-31']\n",
    "# for sweden weekly forecasts are asked\n",
    "print(X.index, Y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3e234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2002-05-05', '2002-05-06', '2002-05-07', '2002-05-08',\n",
      "               '2002-05-09', '2002-05-10', '2002-05-11', '2002-05-12',\n",
      "               '2002-05-13', '2002-05-14',\n",
      "               ...\n",
      "               '2011-12-17', '2011-12-18', '2011-12-19', '2011-12-20',\n",
      "               '2011-12-21', '2011-12-22', '2011-12-23', '2011-12-24',\n",
      "               '2011-12-25', '2011-12-26'],\n",
      "              dtype='datetime64[ns]', name='time', length=3523, freq=None) DatetimeIndex(['2002-06-04', '2002-06-11', '2002-06-18', '2002-06-25',\n",
      "               '2002-07-02', '2002-07-09', '2002-07-16', '2002-07-23',\n",
      "               '2002-07-30', '2002-08-06',\n",
      "               ...\n",
      "               '2011-10-25', '2011-11-01', '2011-11-08', '2011-11-15',\n",
      "               '2011-11-22', '2011-11-29', '2011-12-06', '2011-12-13',\n",
      "               '2011-12-20', '2011-12-27'],\n",
      "              dtype='datetime64[ns]', name='Date', length=500, freq=None)\n",
      "DatetimeIndex(['2014-01-05', '2014-01-06', '2014-01-07', '2014-01-08',\n",
      "               '2014-01-09', '2014-01-10', '2014-01-11', '2014-01-12',\n",
      "               '2014-01-13', '2014-01-14',\n",
      "               ...\n",
      "               '2014-12-20', '2014-12-21', '2014-12-22', '2014-12-23',\n",
      "               '2014-12-24', '2014-12-25', '2014-12-26', '2014-12-27',\n",
      "               '2014-12-28', '2014-12-29'],\n",
      "              dtype='datetime64[ns]', name='time', length=359, freq=None) DatetimeIndex(['2014-02-04', '2014-02-11', '2014-02-18', '2014-02-25',\n",
      "               '2014-03-04', '2014-03-11', '2014-03-18', '2014-03-25',\n",
      "               '2014-04-01', '2014-04-08', '2014-04-15', '2014-04-22',\n",
      "               '2014-04-29', '2014-05-06', '2014-05-13', '2014-05-20',\n",
      "               '2014-05-27', '2014-06-03', '2014-06-10', '2014-06-17',\n",
      "               '2014-06-24', '2014-07-01', '2014-07-08', '2014-07-15',\n",
      "               '2014-07-22', '2014-07-29', '2014-08-05', '2014-08-12',\n",
      "               '2014-08-19', '2014-08-26', '2014-09-02', '2014-09-09',\n",
      "               '2014-09-16', '2014-09-23', '2014-09-30', '2014-10-07',\n",
      "               '2014-10-14', '2014-10-21', '2014-10-28', '2014-11-04',\n",
      "               '2014-11-11', '2014-11-18', '2014-11-25', '2014-12-02',\n",
      "               '2014-12-09', '2014-12-16', '2014-12-23', '2014-12-30'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n",
      "(500, 1) (48, 1) (101, 1)\n",
      "(500, 1) y_train.shape transformed\n",
      "(359, 9)\n",
      "(3523, 9)\n",
      "(730, 9)\n",
      "(48, 30, 9) (500, 30, 9) (101, 30, 9)\n",
      "(48, 1) (500, 1) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:'2011-12-26']\n",
    "Y_train = Y[:'2011-12-27']\n",
    "#print(X_train.index, Y_train.index)\n",
    "\n",
    "X_test = X['2014-01-05':'2014-12-29']\n",
    "Y_test = Y['2014-02-04':'2014-12-30']#29\n",
    "#print(X_test.index, Y_test.index)\n",
    "\n",
    "X_valid = X['2012-01-01':'2013-12-30']\n",
    "Y_valid = Y['2012-01-31':'2013-12-31']\n",
    "#print(Y_train.shape, Y_test.shape, Y_valid.shape)\n",
    "\n",
    "csv_file_path = 'D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge\\data'\n",
    "chdir(csv_file_path)\n",
    "from helper import *\n",
    "\n",
    "(x_tr,y, x_te, y_test, x_va, y_valid, scaler_X,scaler_Y)=prepare_data_weekly(X_train,Y_train,X_valid,Y_valid,X_test,Y_test)\n",
    "\n",
    "iters=3\n",
    "n_steps_in=30\n",
    "n_steps_out=1\n",
    "\n",
    "x = split_sequences_x(x_tr, n_steps_in, n_steps_out)\n",
    "x_test = split_sequences_x(x_te, n_steps_in, n_steps_out)\n",
    "x_valid = split_sequences_x(x_va, n_steps_in, n_steps_out)\n",
    "print(x_test.shape, x.shape, x_valid.shape)\n",
    "print(y_test.shape, y.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c88ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Transformer_HyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(Transformer_HyperModel, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "    def build(self, hp):#statt build_model\n",
    "        head_size = hp.Int(\"head_size\", min_value=8, max_value=64) #, default=16\n",
    "        num_heads = 1\n",
    "        ff_dim = hp.Int(\"ff_dim\", min_value=1, max_value=8) #, default=4\n",
    "        num_transformer_blocks = 2 #, default=4\n",
    "        mlp_units = hp.Int(\"mlp_units\", min_value=10, max_value=200)\n",
    "        dropout = 0.1\n",
    "        mlp_dropout = 0.1\n",
    "        \n",
    "        model = build_model(self.input_shape, self.output_shape, head_size,num_heads,ff_dim,\n",
    "                                num_transformer_blocks,mlp_units,mlp_dropout)\n",
    "        model.compile(loss=\"mean_absolute_error\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),metrics=[\"mean_absolute_error\"])\n",
    "        return model\n",
    "\n",
    "input_shape = x.shape[1:]\n",
    "output_shape = y.shape[1]\n",
    "\n",
    "hyper_model = Transformer_HyperModel(input_shape, output_shape)\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error',restore_best_weights=True, patience=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed3bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 06s]\n",
      "val_mean_absolute_error: 0.1269529014825821\n",
      "\n",
      "Best val_mean_absolute_error So Far: 0.11397659033536911\n",
      "Total elapsed time: 00h 01m 33s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 30, 9)        18          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 30, 9)        1803        layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 30, 9)        0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 30, 9)        0           dropout[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 30, 9)        18          tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 30, 6)        60          layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 30, 6)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 30, 9)        63          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 30, 9)        0           conv1d_1[0][0]                   \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 30, 9)        18          tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 30, 9)        1803        layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30, 9)        0           multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 30, 9)        0           dropout_2[0][0]                  \n",
      "                                                                 tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 30, 9)        18          tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 30, 6)        60          layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 30, 6)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 30, 9)        63          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 30, 9)        0           conv1d_3[0][0]                   \n",
      "                                                                 tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 30)           0           tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 172)          5332        global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 172)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            173         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,429\n",
      "Trainable params: 9,429\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner= kt.RandomSearch(\n",
    "        hyper_model,\n",
    "        overwrite=True,\n",
    "        objective='val_mean_absolute_error',\n",
    "        max_trials = 10,\n",
    "        directory='D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge',\n",
    "        project_name='TF Sweden1'\n",
    "        )\n",
    "\n",
    "tuner.search(\n",
    "        x,\n",
    "        y,\n",
    "        batch_size=24,\n",
    "        epochs=30,\n",
    "        validation_data=(x_valid,y_valid),#achtung test-set hier darf nicht mein standard test set sein, sondern val-set!! \n",
    "        callbacks=[es_callback],\n",
    "        verbose=1)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "Y_pred=best_model.predict(x_test)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4f5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge\\TF Sweden1\n",
      "Showing 10 best trials\n",
      "Objective(name='val_mean_absolute_error', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 46\n",
      "ff_dim: 6\n",
      "mlp_units: 172\n",
      "Score: 0.11397659033536911\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 22\n",
      "ff_dim: 4\n",
      "mlp_units: 117\n",
      "Score: 0.11455947905778885\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 53\n",
      "ff_dim: 7\n",
      "mlp_units: 189\n",
      "Score: 0.11616384983062744\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 27\n",
      "ff_dim: 6\n",
      "mlp_units: 105\n",
      "Score: 0.1167249009013176\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 43\n",
      "ff_dim: 3\n",
      "mlp_units: 36\n",
      "Score: 0.11826717853546143\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 53\n",
      "ff_dim: 6\n",
      "mlp_units: 51\n",
      "Score: 0.11877071857452393\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 8\n",
      "ff_dim: 5\n",
      "mlp_units: 99\n",
      "Score: 0.1269529014825821\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 38\n",
      "ff_dim: 4\n",
      "mlp_units: 57\n",
      "Score: 0.1357397586107254\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 38\n",
      "ff_dim: 6\n",
      "mlp_units: 160\n",
      "Score: 0.14684322476387024\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "head_size: 24\n",
      "ff_dim: 3\n",
      "mlp_units: 76\n",
      "Score: 0.14771342277526855\n",
      "0.25773512522379544\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()\n",
    "\n",
    "csv_file_path = 'D:\\Arbeit PhD\\Fachlich\\z_Sonstiges\\Groundwater challenge'\n",
    "chdir(csv_file_path)\n",
    "from tensorflow.keras.models import save_model\n",
    "best_model.save('TF_Sweden1_tune.h5')\n",
    "\n",
    "NSE=np.array([[None]*n_steps_out])\n",
    "MAE=np.array([[None]*n_steps_out])\n",
    "RMSE=np.array([[None]*n_steps_out])\n",
    "SAPE=np.array([[None]*n_steps_out])\n",
    "\n",
    "Y_pred_test=best_model.predict(x_test)\n",
    "Y_pred_valid=best_model.predict(x_valid)\n",
    "\n",
    "y_correct_utf = scaler_Y.inverse_transform(y_test)#test\n",
    "yhat_utf = scaler_Y.inverse_transform(Y_pred_test)#test\n",
    "Y_pred_valid_utf = scaler_Y.inverse_transform(Y_pred_valid)#valid\n",
    "\n",
    "NSE[:]= np.array([1 - sum((yhat_utf[:,i]-y_correct_utf[:,i])**2)/sum((y_correct_utf[:,i]-np.mean(y_correct_utf[:,i]))**2)\n",
    "                  for i in range(n_steps_out)])\n",
    "MAE[:]=np.array([mean_absolute_error(y_correct_utf[:,i], yhat_utf[:,i]) for i in range(n_steps_out)])\n",
    "RMSE[:]=np.array([np.sqrt(np.mean((yhat_utf[:,i]-y_correct_utf[:,i])**2)) for i in range(n_steps_out)])\n",
    "SAPE[:] = np.array([np.abs((yhat_utf[:,i] - y_correct_utf[:,i])/(0.5*(y_correct_utf[:,i]+yhat_utf[:,i]))).mean()\n",
    "                         for i in range(n_steps_out)]) #mean absolute percent error\n",
    "\n",
    "print(np.mean(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e440e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27feac42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307092c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41765479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2dfcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8362c94acb70529fb597ced4e020ad9ec7f0e835666f207c4442104931f7034b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
